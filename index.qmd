---
title: Summer is Coming
subtitle: AI Tools for R, Shiny, and Pharma
author: Joe Cheng
date: 2024-10-29
format:
  revealjs:
    theme: simple
    transition: slide
    slide-number: true
    chalkboard: true
    incremental: true
editor:
  render-on-save: true
---

```{r setup,include=FALSE}
options(cli.width = 68)
```

## It's easy to be skeptical about GenAI

* Grandiose claims about what LLMs can do
* Even more grandiose claims about what they will become in the near future
* The e/acc movement ðŸ™„

## Meanwhile, in the real world

* ChatGPT tells us plausible falsehoods
* Copilot writes code that often doesn't work
* Facebook is flooded with AI generated nonsense
* Is this even the future we want?

## Why I think "Summer is Coming"

* Judging LLMs by ChatGPT is like judging the iPhone by its phone app
    * Put compute, touchscreen, sensors, and connectivity in billions of peoples' hands, and interesting things happen
* The real power of LLMs is in their APIs!
    * Every programmer now has access to a magic function that approximates human reasoning!

## Agenda

1. Demo
2. How to call LLMs from R
3. Using LLMs responsibly

# Demos

## Demo 1: Sidebot

[https://github.com/jcheng5/r-sidebot](https://github.com/jcheng5/r-sidebot)

## Demo 2: Sidebot for Pharma

[https://github.com/jcheng5/pharma-sidebot](https://github.com/jcheng5/pharma-sidebot)

## RStudio IDE integrations

::: {.nonincremental}
* {pal} by Simon Couch\
[https://simonpcouch.github.io/pal/](https://simonpcouch.github.io/pal/)

* {chattr} by Edgar Ruiz\
[https://mlverse.github.io/chattr/](https://mlverse.github.io/chattr/)

* {gptstudio}/{gpttools} by Michel Nivard, James Wade, and Samuel Calderon\
[https://michelnivard.github.io/gptstudio/](https://michelnivard.github.io/gptstudio/)\
[https://jameshwade.github.io/gpttools/](https://jameshwade.github.io/gpttools/)
:::

## Other experiments

::: {.nonincremental}
* Turn recipe e-books and web pages into structured (JSON) data
* GitHub issue auto-labeler, based on project-specific criteria
* Automated ALT text for plots (for visually impaired users)
* Code linter and security analyzer, for almost any language
* Mock dataset generator
* Natural language processing: detecting locations in news articles, extracting emotions from reviews
:::

## Negative results

::: {.nonincremental}
* Look at raw data and summarize/interpret it (without offloading to R or Python)
* Generate complicated regular expressions
* Replace technical documentation for Posit's commercial products
:::

# How to use LLMs from R

## Introducing {elmer}

::: {.nonincremental}
[https://hadley.github.io/elmer/](https://hadley.github.io/elmer/)

- A new package for R for working with chat APIs
- **Easy:** Might be the easiest LLM API client in any language
- **Powerful:** Designed for multi-turn conversations, streaming, async, and tool calling
- **Compatible:** Works with OpenAI, Anthropic, Google, AWS, Ollama, Perplexity, Groq, and more
:::

(Prior art: {openai}, {tidyllm}, {gptr}, {rgpt3}, {askgpt}, {chatgpt}...)

## Getting started

```{r}
#| echo: true
#| cache: true

# Assumes $OPENAI_API_KEY is set

chat <- elmer::chat_openai(
  model = "gpt-4o",
  system_prompt = "Be terse but professional."
)

chat$chat("When was the R language created?")
```

. . .

```{r}
#| echo: true
#| cache: true

# The `chat` object keeps the conversation history
chat$chat("Who created it?")
```

## Tool Calling

[https://hadley.github.io/elmer/articles/tool-calling.html](https://hadley.github.io/elmer/articles/tool-calling.html)

- Level up your chatbot with R functions
- Write R functions, and expose them to the chatbot
- The chatbot can call these functions with arguments, and use the results in its responses

## Example

**Step 1:** Create an R function (or "tool") for the chatbot to call

```{r}
#| echo: true
#| cache: true
library(openmeteo)

#' Get current weather data from Open-Meteo using openmeteo package
#'
#' @param lat The latitude of the location.
#' @param lon The longitude of the location.
#' @return A list containing current weather information including temperature (F), wind speed (mph), and precipitation (inch).
get_current_weather <- function(lat, lon) {
  openmeteo::weather_now(
    c(lat, lon),
    response_units = list(temperature_unit = "fahrenheit", windspeed_unit = "mph", precipitation_unit = "inch")
  ) |> jsonlite::toJSON(auto_unbox = TRUE)
}
```

## Example

**Step 2:** Register the function/tool with the chatbot

```{r}
#| echo: true
#| cache: true
library(elmer)
chat <- chat_openai(model = "gpt-4o")

chat$register_tool(tool(
  get_current_weather,
  "Get current weather data from Open-Meteo using openmeteo package. Returns a list containing current weather information including temperature (F), wind speed (mph), and precipitation (inch).",
  lat = type_number("The latitude of the location."),
  lon = type_number("The longitude of the location.")
))
```

. . .

You don't have to write this code by hand; `elmer::create_tool_def(get_current_weather)` generated this.

## Example

**Step 3:** Ask the chatbot a question that requires the tool

```{r}
#| echo: true
#| cache: true
chat$chat("What's the weather at Fenway Park?")
```

## Example

```{r}
#| echo: true
#| cache: true
chat
```

## How It Works {visibility="hidden"}

::: {.incremental}
- [Not like this](https://sequencediagram.org/index.html?presentationMode=readOnly&shrinkToFit=true#initialData=C4S2BsFMAIAkHsDu1j3uaBjAhucIA7Ac2gBEB5AUQGUA5QTAIAVaReAJwGsAoLgM3gLBsiSAGd4AW0gA2AEZtsBACajovACy8A7NErgpbLhOzBIbELiViQRAiEwDVAMQCM0rQEFo5AA6QCHgCSRiZmFuBWojZ2DgTOAAwArACcidDwfgQiJgAWZgB07EQ8egYAtAB8vv5BAFzQAN4AOgTQ0GzokPUARACuombdADQtbbGmgj0A6jkmAOSqwHkskLlm7TY5wNAESNAm0E7+iNgAntAACticAPzDLQC+XNUBgZUZx6tLBUX1AOKUZgAemy3zYQMwvTYbH8wCB-SB8QATEiXIl8gArcQELgfLJfPJsQpsIiVF51RrdUwSHzderSRJDaDdWJKMAgRx05miXoEAinbpPclvCqlMz1ZqtdqdHrYURRURCQT3KXjWE9QLABbQBnQKxEGFifbKaA8vmnQAoBNhoLJVr1QLxehglGcAITdR5AA) - with the assistant executing stuff
- [Yes like this](https://sequencediagram.org/index.html?presentationMode=readOnly&shrinkToFit=true#initialData=C4S2BsFMAIAkHsDu1j3uaBjAhucIA7Ac2gBEB5AUQGVpF4AnAawCgWBbbYSBkXAE0gBnEEQIhM8AkOgAxAAwBWAJyLo8AA6QCiSFwAWPAHSMiLAGZTg2XUPjtIANgBGDbAX4zzAFnMB2aEpwBwYOLh4+cEERMQkpGVkARkc-AEFoci0CVIBJNiCQgFoAPkztXIAuaABvAB0CaGgGdEgqgCIAVyEeNoAaesbJAm5h9oB1fS4AchlgQzo9OZ4m0X1gaAIkaC45bURsAE9oAAVsZgB+PoGUNHAhKoBtIkhgAH1MDoYGbTfdAx4ALr1AC+LDK2RyJQKPCqAGFUgAZBHQZ5vD5fH6vP5LBgACja8gATITEoo2gBKFjQhglTR7RaGBgmBhEKoAcUoABVoAB6bGMnno77DHldHlEkmKIwAKzsBBYdJ0DOMpihwRhNTa3HYGjaVUcil60DaQ34YBA8T1xqEHQIBAObVB1JK4Mq0AASlyAKrugBymu1uv1huNpvNlvaNrtDtBrshxWpVTqDSaLXa2CEIiE1mGVxTQxGwHaOWAM2gBuggiI32E2w80Cj9sAKATYaDOPQdUDmDoYfiHACEbRBQA)
  - Elmer asks LLM a question; includes metadata for available tools
  - LLM asks Elmer to invoke a tool, passing its desired arguments
  - Elmer invokes the tool, and returns the output to the LLM
  - LLM incorporates the tool's output as additional context for formulating a response
:::

## How It Works

Another way to think of it:

- The client can perform tasks that the assistant can't do
- Tools put control into the hands of the assistantâ€”it decides when to use them, and what arguments to pass in, and what to do with the results
- Having an "intelligent-ish" coordinator of tools is a surprisingly general, powerful capability!

## Introducing {shinychat}

[https://github.com/jcheng5/shinychat](https://github.com/jcheng5/shinychat)

* Chatbot UI for Shiny for R
* Designed to integrate with {elmer} or bring any other chat client

## Introducing {shinychat}

```{.r code-line-numbers="|5|9|11|12-13|"}
library(shiny)
library(shinychat)

ui <- bslib::page_fluid(
  chat_ui("chat")
)

server <- function(input, output, session) {
  chat <- elmer::chat_openai(system_prompt = "You're a trickster who answers in riddles")
  
  observeEvent(input$chat_user_input, {
    stream <- chat$stream_async(input$chat_user_input)
    chat_append("chat", stream)
  })
}

shinyApp(ui, server)
```

## Putting it all together

- Create a Shiny interface using {shinychat}
- Converse using {elmer}
- Register tools with {elmer} to control parts of your Shiny app (e.g., from the [Pharma Sidebot example](https://github.com/jcheng5/pharma-sidebot/blob/de0b5506824d1ad9a4ef9c105f5c32dce0f7b455/app.R#L169-L196))

# Using LLMs responsibly

## What are the dangers?

1. Incorrect and unverifiable answers
2. Uneven reasoning, math, and coding capabilities
3. Lack of interpretability
4. Lack of reproducibility
5. Data security/privacy

## Incorrect and unverifiable answers {.smaller}

1. **Just say no** if correctness is paramount.
2. **Keep a human in the loop:** Allow the user to inspect not just the answers, but the method(s) used by the model to get to the answers.
3. **Automatically verify the LLM's answers:** Find e.g. syntax errors in generated code and send it back if necessary.
4. **Use cases with "squishy" answers:** If correctness is subjective, might be a good fit for LLMs.
5. **Set clear expectations with the user** by making it obvious where cool but unreliable technology is being used.

## Uneven reasoning, math, and coding capabilities

1. **Check your own assumptions** empirically by thoroughly testing the use cases you care about. Use the same model/prompt (and if possible, data) you'll use in production.
2. **Outsource math/stats calculations** to tool calls.
3. **Set clear expectations with your users** that the tool is fallible and some perseverance is required.

## Data security/privacy

* Many companies are loathe to send queries (laden with potentially proprietary code and other trade secrets) to OpenAI and Anthropic.

* "Open" models like Llama are safer, but aren't as smart (yet).

* AWS-hosted Anthropic models and Azure-hosed OpenAI models may be helpful.

# Thank you

::: {.noninteractive}
* {elmer}: [https://hadley.github.io/elmer/](https://hadley.github.io/elmer/)
* {shinychat}: [https://github.com/jcheng5/shinychat](https://github.com/jcheng5/shinychat)
:::
